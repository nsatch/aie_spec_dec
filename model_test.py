##### CODE ADAPTED FROM: https://github.com/uw-mad-dash/decoding-speculative-decoding/blob/main/spec_decoding_deployment.py #####

import torch
import torch.nn as nn
import re
from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM
import deepspeed
import torch.distributed as dist
import json
import os


def json_loader(file_path):
    """
    Input: file_path: Path to the json file containing all the queries.

    File format looks like the following:
    {"prompt": "A seated man cleans a shoe in a classroom setting with other individuals. the man"}
    {"prompt": "Two girls are sailing on a lake. they"}

    Output: This function returns a list of prompts to be used by the draft LLM.
    """
    data = []
    with open(file_path, 'r') as file:
        for line in file:
            data.append(json.loads(line.strip()))
    return data

def oracle_verification(model, input_tensor, max_new_tokens, local_rank, iter_count, past_key_values):
    """
    Verifies the predictions of an oracle model by comparing the generated tokens against actual tokens.

    Args:
        model (torch.nn.Module): The oracle model used for generating predictions.
        input_tensor (torch.Tensor): The input tensor containing tokens for prediction.
        max_new_tokens (int): The maximum number of new tokens to be generated by the model.
        local_rank (int): The local rank identifier for distributed training.
        iter_count (int): The current iteration count, used to determine the first call to the model.
        past_key_values (torch.Tensor): Cached past key values for accelerating generation in subsequent calls.

    Returns:
        tuple: A tuple containing:
               - The positions of the first incorrect predictions in each row.
               - The updated past_key_values tensor with dimensions adjusted based on the first incorrect prediction.
    """

    # If it's the first iteration, generate outputs using the full input tensor
    if iter_count == 1:
        outputs = model(input_tensor.unsqueeze(0).cuda(local_rank), use_cache=True)
        new_past_key_values = outputs.past_key_values

        # Extract the token IDs predicted for the last max_new_tokens positions
        next_token_id = outputs.logits[:, -max_new_tokens - 1:-1, :].argmax(dim=-1, keepdim=False)
    else:
        # For subsequent iterations, use past key values to accelerate generation
        outputs = model(input_tensor.unsqueeze(0).cuda(local_rank), past_key_values=past_key_values, use_cache=True)
        new_past_key_values = outputs.past_key_values
        next_token_id = outputs.logits[:, :, :].argmax(dim=-1, keepdim=False)
    
    print()
    print("ORACLE VERIFICATION FUNCTION")
    print("new_past_key_values size: ")
    print(len(new_past_key_values))
    print(len(new_past_key_values[0]))
    print(new_past_key_values[0][0].size())
    # for i, layer_past_key_value in enumerate(new_past_key_values):
    #     print(f"Layer {i} size: {layer_past_key_value.size()}")
    print("next_token_id: ", next_token_id)

    # Extract the actual next tokens from the input tensor
    actual_next_tokens_tensor = input_tensor[-max_new_tokens:]

    print("actual_next_tokens_tensor: ", actual_next_tokens_tensor)

    # Compare the predicted next tokens with the actual next tokens
    correct_predictions = (next_token_id == actual_next_tokens_tensor)

    # Convert the boolean tensor to a float tensor for further processing
    correct_predictions_float = correct_predictions.float()

    print("correct predictions: ", correct_predictions_float)

    # Initialize a tensor to hold the positions of the first incorrect prediction in each row
    
    # Find the position of the first incorrect token
    first_false_position = torch.where(~correct_predictions)[1].min(dim=-1, keepdim=True).values if not correct_predictions.all() else torch.full_like(correct_predictions[0], max_new_tokens)
    # first_false_positions = torch.full((correct_predictions_float.size(0),), correct_predictions_float.size(1),
    #                                device=correct_predictions_float.device)
    # first_false_positions = torch.full((correct_predictions_float.size(0),), correct_predictions_float.size(1),
    #                                    device=correct_predictions_float.device)
    
    print("first false position: ", first_false_position)

    # Check if there's any incorrect prediction within the max_new_tokens limit and adjust past_key_values accordingly
    if first_false_position < max_new_tokens:
        updated_past_key_values = []

        for layer_idx in range(len(new_past_key_values)):
            new_key_tensor = new_past_key_values[layer_idx][0][:, :, :-max_new_tokens + first_false_position, :]
            new_value_tensor = new_past_key_values[layer_idx][1][:, :, :-max_new_tokens + first_false_position, :]
            
            updated_layer_tuple = (new_key_tensor, new_value_tensor)

            updated_past_key_values.append(updated_layer_tuple)
            # new_past_key_values[layer_idx] = (
            #     new_past_key_values[layer_idx][0][:, :, :-max_new_tokens + first_false_position, :],
            #     new_past_key_values[layer_idx][1][:, :, :-max_new_tokens + first_false_position, :]
            # )

        print("updated key value size: ")
        print(len(new_past_key_values))
        print(len(new_past_key_values[0]))
        print(new_past_key_values[0][0].size())

        return first_false_position, tuple(updated_past_key_values)

    return first_false_position, new_past_key_values


if __name__ == "__main__":
    dist.init_process_group(backend='nccl')
    world_size = dist.get_world_size()
    rank = dist.get_rank()
    local_rank = int(os.environ["LOCAL_RANK"])
    torch.cuda.set_device(local_rank)
    device = torch.device("cuda", local_rank)

    print(world_size, rank, local_rank)
    os.environ['TRANSFORMERS_CACHE'] = "cache"  # Replace with your transformers cache directory

    test_json = json_loader("./hellaswag_shortened.json")  # Replace with your file path

    ### comment out after model/tokenizer is generated ###
    # rewrite_embedding(model_name='meta-llama/Llama-3.2-3B', save_path='llama-3_2-3b')

    # Define the checkpoint dict. You may need to convert *.safetensors to
    # *.bin for this work. Make sure you get all the *.bin and *.pt files in
    # the checkpoint_files list.
    checkpoint_dir = "ckpt"

    # Change ckpt names if your .bin files are named differently
    checkpoint_files = [
        os.path.join(checkpoint_dir, f"pytorch_model-{i:05d}-of-00029.bin")
        for i in range(1, 30)  # Change number of bin files based on your model
    ]

    checkpoint_dict = {
        "type": "DS_MODEL",
        "checkpoints": checkpoint_files,
        "version": 1.0,
    }

    oracle_model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-3B', torch_dtype=torch.float16)
    # <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'

    oracle_model = deepspeed.init_inference(
        oracle_model,
        replace_with_kernel_inject=False,
        # tp={"tp_size": tensor_parallel_degrees, },
        tp={"tp_size": world_size, },
        dtype=torch.float16,
        checkpoint=checkpoint_dict,
    )
    # <class 'deepspeed.inference.engine.InferenceEngine'> 

    # The LLaMA tokenizer does not have a pad token.
    # Modify the tokenizer to add a pad token and change the model configs accordingly.
    tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-3B', torch_dtype=torch.float16)

    # Feel free to change it to the draft model of your choice
    draft_model = AutoModelForCausalLM.from_pretrained("minghaoyan/Wide-Sheared-LLaMA-796M", torch_dtype=torch.float16)
    draft_model.resize_token_embeddings(len(tokenizer))


    # Launch the draft model with deepspeed on 1 node. Alternatively, you could use HF or load from a checkpoint.
    draft_model = deepspeed.init_inference(
        draft_model,
        replace_with_kernel_inject=False,
        tp={"tp_size": 1, },
        dtype=torch.float16,
        # checkpoint=checkpoint_dict,
    )

    # Set hyperparameters for speculative decoding
    max_tokens = 7
    output_file = "3b_796m_shortened.txt"
    past_key_values = None

    iteration = 0
    for data in test_json:
        if iteration == 0:
            current_prompt = data["prompt"]

            print("Current prompt: ", current_prompt)

            draft_input_ids = tokenizer.encode(current_prompt, return_tensors="pt").cuda(local_rank)

            print("Number of draft ids: ", draft_input_ids.size())

            # Calculating the maximum length for the generated sequence
            max_length = 200 - max_tokens - 2
            current_length = 0
            iter_count = 0

            # Intialize tensor to keep track of total matched tokens
            total_matched = torch.zeros(1, dtype=torch.int32).cuda(local_rank)

            while current_length < max_length:
                print("Iteration: ", current_length, max_length)

                if iter_count == 0: 
                    # For the first iteration, use the input prompt
                    iter_count += 1
                    input_tensor = draft_input_ids
                else: 
                    # For subsequent iterations, use new inputs based on matched tokens
                    input_tensor = new_inputs

                print("Input tensor size: ", input_tensor.size(1))
                print("Input tensor: ", input_tensor)

                output_len = input_tensor.size(1) + max_tokens

                # Generate predictions
                generated_tokens = draft_model.generate(input_tensor, max_new_tokens=max_tokens).to(dtype=torch.int32)

                # Verifying the generated sequence against the ground truth
                first_false_position, past_key_values = oracle_verification(
                    oracle_model, generated_tokens.squeeze(0), max_tokens, local_rank, iter_count, past_key_values
                )
                
                # matched_tokens = first_false_position
                matched_tokens = first_false_position + 1
                print("Matched tokens: ", matched_tokens)

                new_inputs = torch.cat([input_tensor[:, :matched_tokens], generated_tokens[:, :matched_tokens]], dim=-1)

                print("New inputs: ", new_inputs)

                total_matched += matched_tokens
                print("Total matched: ", total_matched)

                # Save results
                if local_rank == 0:
                    with open(output_file, "a") as f:
                        f.write(str(total_matched.tolist()) + "\n")

                current_length = total_matched.item()

            iteration += 1
        else:
            break
