##### CODE ADAPTED FROM: https://github.com/uw-mad-dash/decoding-speculative-decoding/blob/main/spec_decoding_deployment.py #####

import torch
import torch.nn as nn
import re
from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM
import deepspeed
import torch.distributed as dist
import json
import os


def json_loader(file_path):
    """
    Input: file_path: Path to the json file containing all the queries.

    File format looks like the following:
    {"prompt": "A seated man cleans a shoe in a classroom setting with other individuals. the man"}
    {"prompt": "Two girls are sailing on a lake. they"}

    Output: This function returns a list of prompts to be used by the draft LLM.
    """
    data = []
    with open(file_path, 'r') as file:
        for line in file:
            data.append(json.loads(line.strip()))
    return data


def rewrite_embedding(model_name, save_path):
    """
    This function rewrites your target LLM to include [PAD] token, which does not exist in the original LLaMA/LLaMA-2 tokenizer.
    """
    access_token = "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"    

    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side="left", token=access_token, cache_dir=os.environ['TRANSFORMERS_CACHE'])
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    tokenizer.pad_token = "[PAD]"
    tokenizer.padding_side = "left"
    tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)

    def tokenize_function(examples):
        return tokenizer.batch_encode_plus(examples, padding="longest")['input_ids']

    oracle_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)
    oracle_model.resize_token_embeddings(len(tokenizer))
    oracle_model.config.pad_token_id = tokenizer.pad_token_id
    oracle_model.embed_tokens = nn.Embedding(oracle_model.config.vocab_size, oracle_model.config.hidden_size,
                                             padding_idx=oracle_model.config.pad_token_id)

    oracle_model.save_pretrained(save_path)
    tokenizer.save_pretrained(save_path)

    print("done")

def oracle_verification(model, input_tensor, max_new_tokens, local_rank, iter_count, past_key_values):
    """
    Verifies the predictions of an oracle model by comparing the generated tokens against actual tokens.

    Args:
        model (torch.nn.Module): The oracle model used for generating predictions.
        input_tensor (torch.Tensor): The input tensor containing tokens for prediction.
        max_new_tokens (int): The maximum number of new tokens to be generated by the model.
        local_rank (int): The local rank identifier for distributed training.
        iter_count (int): The current iteration count, used to determine the first call to the model.
        past_key_values (torch.Tensor): Cached past key values for accelerating generation in subsequent calls.

    Returns:
        tuple: A tuple containing:
               - The positions of the first incorrect predictions in each row.
               - The updated past_key_values tensor with dimensions adjusted based on the first incorrect prediction.
    """

    # If it's the first iteration, generate outputs using the full input tensor
    if iter_count == 1:
        outputs = model(input_tensor.unsqueeze(0).cuda(local_rank), use_cache=True)
        new_past_key_values = outputs.past_key_values

        # Extract the token IDs predicted for the last max_new_tokens positions
        next_token_id = outputs.logits[:, -max_new_tokens - 1:-1, :].argmax(dim=-1, keepdim=False)
    else:
        # For subsequent iterations, use past key values to accelerate generation
        outputs = model(input_tensor.unsqueeze(0).cuda(local_rank), past_key_values=past_key_values, use_cache=True)
        new_past_key_values = outputs.past_key_values
        next_token_id = outputs.logits[:, :, :].argmax(dim=-1, keepdim=False)
    
    print()
    print("ORACLE VERIFICATION FUNCTION")
    print("new_past_key_values size: ")
    print(len(new_past_key_values))
    print(len(new_past_key_values[0]))
    print(new_past_key_values[0][0].size())
    # for i, layer_past_key_value in enumerate(new_past_key_values):
    #     print(f"Layer {i} size: {layer_past_key_value.size()}")
    print("next_token_id: ", next_token_id)

    # Extract the actual next tokens from the input tensor
    actual_next_tokens_tensor = input_tensor[-max_new_tokens:]

    print("actual_next_tokens_tensor: ", actual_next_tokens_tensor)

    # Compare the predicted next tokens with the actual next tokens
    correct_predictions = (next_token_id == actual_next_tokens_tensor)

    # Convert the boolean tensor to a float tensor for further processing
    correct_predictions_float = correct_predictions.float()

    print("correct predictions: ", correct_predictions_float)

    # Initialize a tensor to hold the positions of the first incorrect prediction in each row
    
    # Find the position of the first incorrect token
    first_false_position = torch.where(~correct_predictions)[1].min(dim=-1, keepdim=True).values if not correct_predictions.all() else torch.full_like(correct_predictions[0], max_new_tokens)
    # first_false_positions = torch.full((correct_predictions_float.size(0),), correct_predictions_float.size(1),
    #                                device=correct_predictions_float.device)
    # first_false_positions = torch.full((correct_predictions_float.size(0),), correct_predictions_float.size(1),
    #                                    device=correct_predictions_float.device)
    
    print("first false position: ", first_false_position)

    # Check if there's any incorrect prediction within the max_new_tokens limit and adjust past_key_values accordingly
    if first_false_position < max_new_tokens:
        updated_past_key_values = []

        for layer_idx in range(len(new_past_key_values)):
            new_key_tensor = new_past_key_values[layer_idx][0][:, :, :-max_new_tokens + first_false_position, :]
            new_value_tensor = new_past_key_values[layer_idx][1][:, :, :-max_new_tokens + first_false_position, :]
            
            updated_layer_tuple = (new_key_tensor, new_value_tensor)

            updated_past_key_values.append(updated_layer_tuple)
            # new_past_key_values[layer_idx] = (
            #     new_past_key_values[layer_idx][0][:, :, :-max_new_tokens + first_false_position, :],
            #     new_past_key_values[layer_idx][1][:, :, :-max_new_tokens + first_false_position, :]
            # )

        print("updated key value size: ")
        print(len(new_past_key_values))
        print(len(new_past_key_values[0]))
        print(new_past_key_values[0][0].size())

        return first_false_position, tuple(updated_past_key_values)

    return first_false_position, new_past_key_values


if __name__ == "__main__":
    dist.init_process_group(backend='nccl')
    world_size = dist.get_world_size()
    rank = dist.get_rank()
    local_rank = int(os.environ["LOCAL_RANK"])
    torch.cuda.set_device(local_rank)
    device = torch.device("cuda", local_rank)

    print(world_size, rank, local_rank)
    os.environ['TRANSFORMERS_CACHE'] = "cache"  # Replace with your transformers cache directory

    test_json = json_loader("./hellaswag_shortened.json")  # Replace with your file path

    ### comment out after model/tokenizer is generated ###
    # rewrite_embedding(model_name='meta-llama/Llama-3.2-3B', save_path='llama-3_2-3b')

    torch.cuda.empty_cache()

    # Set your target LLM and desired tp degrees here.
    model_name = "llama-3_2-3b"
    # tensor_parallel_degrees = 4

    # Load the model on meta tensors from the config file.
    # This prevents deepspeed from loading models multiple times on each rank.
    config = AutoConfig.from_pretrained(model_name, cache_dir="cache")

    with deepspeed.OnDevice(dtype=torch.float16, device="meta", enabled=True):
        oracle_model = AutoModelForCausalLM.from_config(config, torch_dtype=torch.float16)

    # Define the checkpoint dict. You may need to convert *.safetensors to
    # *.bin for this work. Make sure you get all the *.bin and *.pt files in
    # the checkpoint_files list.
    checkpoint_dir = "ckpt"

    # Change ckpt names if your .bin files are named differently
    checkpoint_files = [
        os.path.join(checkpoint_dir, f"pytorch_model-{i:05d}-of-00029.bin")
        for i in range(1, 30)  # Change number of bin files based on your model
    ]

    checkpoint_dict = {
        "type": "DS_MODEL",
        "checkpoints": checkpoint_files,
        "version": 1.0,
    }

    oracle_model = deepspeed.init_inference(
        oracle_model,
        replace_with_kernel_inject=False,
        # tp={"tp_size": tensor_parallel_degrees, },
        tp={"tp_size": world_size, },
        dtype=torch.float16,
        checkpoint=checkpoint_dict,
    )

    # The LLaMA tokenizer does not have a pad token.
    # Modify the tokenizer to add a pad token and change the model configs accordingly.
    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side="left",
                                              torch_dtype=torch.float16)
    tokenizer.pad_token = "[PAD]"

    # Feel free to change it to the draft model of your choice
    draft_model = AutoModelForCausalLM.from_pretrained("minghaoyan/Wide-Sheared-LLaMA-796M", torch_dtype=torch.float16)

    draft_model.resize_token_embeddings(len(tokenizer))
    draft_model.config.pad_token_id = tokenizer.pad_token_id
    draft_model.embed_tokens = nn.Embedding(draft_model.config.vocab_size, draft_model.config.hidden_size,
                                            padding_idx=draft_model.config.pad_token_id)

    # Launch the draft model with deepspeed on 1 node. Alternatively, you could use HF or load from a checkpoint.
    draft_model = deepspeed.init_inference(
        draft_model,
        replace_with_kernel_inject=False,
        tp={"tp_size": 1, },
        dtype=torch.float16,
        # checkpoint=checkpoint_dict,
    )

    # Set hyperparameters for speculative decoding
    batch_size = 1
    max_new_tokens = 7
    output_file = "3b_796m_shortened.txt"
    past_key_values = None

    print(draft_model.config)
    print(draft_model.config.vocab_size)

    print("Pad token ID: ", tokenizer.pad_token_id)
    print("Eos token ID: ", tokenizer.eos_token_id)

    iteration = 0
    for batch in test_json:
        if iteration == 0:
            current_prompt = batch["prompt"]

            print("Current prompt: ", current_prompt)

            draft_input_ids = tokenizer.encode(current_prompt, return_tensors="pt").cuda(local_rank)

            print("Number of draft ids: ", draft_input_ids.size())

            # Calculating the maximum length for the generated sequence
            max_length = 200 - max_new_tokens - 2
            current_length = 0
            iter_count = 0

            # Intialize tensor to keep track of total matched tokens
            total_matched = torch.zeros(1, dtype=torch.int32).cuda(local_rank)

            # while current_length < max_length:
            print("Iteration: ", current_length, max_length)

            if iter_count == 0: 
                # For the first iteration, use the input prompt
                iter_count += 1
                input_tensor = draft_input_ids
            else: 
                # For subsequent iterations, use new inputs based on matched tokens
                input_tensor = new_inputs

            print("Input tensor size: ", input_tensor.size(1))

            output_len = input_tensor.size(1) + max_new_tokens

            # Generate predictions
            generated_tokens = draft_model.generate(input_tensor, max_new_tokens=max_new_tokens,
                                                                pad_token_id=tokenizer.eos_token_id).to(dtype=torch.int32)

            print("Generated tokens: ", generated_tokens)

            outputs = oracle_model(generated_tokens)
            logits = outputs.logits
            print("Logits from target: ", logits)

            # # Verifying the generated sequence against the ground truth
            # first_false_position, past_key_values = oracle_verification(
            #     oracle_model, generated_tokens.squeeze(0), max_new_tokens, local_rank, iter_count, past_key_values
            # )
            
            # # matched_tokens = first_false_position
            # matched_tokens = first_false_position + 1
            # print("Matched tokens: ", matched_tokens)

            # new_inputs = torch.cat([input_tensor[:, :matched_tokens], generated_tokens[:, :matched_tokens]], dim=-1)

            # print("New inputs: ", new_inputs)

            # total_matched += matched_tokens
            # print("Total matched: ", total_matched)

            # # Save results
            # if local_rank == 0:
            #     with open(output_file, "a") as f:
            #         f.write(str(total_matched.tolist()) + "\n")

            # current_length = total_matched.item()

            iteration += 1


            