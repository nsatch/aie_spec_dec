import sys
from colors import *
from sort_prompts import *
from config_and_setup import *
from set_up_test import *
from post_processing import *

from time import time

import subprocess

# Save output logs
RUN_SCRIPT = "spec_decoding_final_param.py"
# TODO: I need a better way of doing th is . . 
OUTPUT_DIR = "benchmarking/output_logs"
OUTPUT_DIR2 = OUTPUT_DIR.replace("benchmarking/", "")

CONFIG_FILE_LOC = "config.yaml"
if __name__ == "__main__":
    verify_venv()
    cprint(YELLOW, "\nGrabbing config file")
    config = parse_config_file(CONFIG_FILE_LOC)

    # TODO: Record time to run whole test suite
    # Construct input prompt list (get n input prompts of varying lengths) & point to new file in param code
    cprint(YELLOW, "\nConstructing inputs")
    strided_input_prompts = create_strided_prompts_from_config(config)
    update_input_file_in_param_code(strided_input_prompts, RUN_SCRIPT)
    # TODO: Update the number of speculative tokens generated by drafter
    # TODO: Print the above number for logging purposes

    cprint(YELLOW, "\nConstructing output results directory")
    OUTPUT_DIR2 = construct_output_directory(config, OUTPUT_DIR2)
    OUTPUT_DIR = os.path.join("benchmarking", OUTPUT_DIR2)

    # Construct test (or construct all tests at once and write into diff files?)
    # Run the test and collect its output; time runtime
    cprint(YELLOW, f"\nConstructing test suite")
    tests = create_test_suite(config)
    cprint(CYAN, "\nStarting tests")
    # TODO: Link test to test_name via class?
    test_suite_start = time()
    for i, testcase in enumerate(tests):
        cprint(YELLOW, f"\nConstructing test {i+1} of {len(tests)}")
        construct_test(testcase, RUN_SCRIPT)

        cprint(YELLOW, f"\nRunning test {i+1} of {len(tests)}")
        # TODO: Print test details
        #   - Drafter enabled? What are drafters, input prompt number, size of main model
        #test_name = "test2"
        test_name = config['test_name'] + str(i)
        output_log = os.path.join(OUTPUT_DIR, test_name) + ".log"
        output_error_log = os.path.join(OUTPUT_DIR, test_name + "_error") + ".log"
        output_log2 = os.path.join(OUTPUT_DIR2, test_name) + ".log"
        start = time()
        cprint(MAGENTA, f"\tNum prompts: {config['num_input_prompts']}")
        subprocess.run(f"cd .. && deepspeed --num_gpus 1 {RUN_SCRIPT} >{output_log} 2>{output_error_log}", shell=True)
        # Figure out how to still print to terminal while piping
        #subprocess.run(f"cd .. && deepspeed --num_gpus 1 {RUN_SCRIPT}", shell=True)
        #subprocess.run(f"cd .. && deepspeed --num_gpus 1 {RUN_SCRIPT} >{output_log}", shell=True)
        #subprocess.run(f"cd .. && deepspeed --num_gpus 1 {RUN_SCRIPT} > >(tee {output_log}) 2> >(tee {output_error_log} >&2)", shell=True)
        #subprocess.run(f"echo 'Return status: '$? >> {output_log2}", shell=True)
        end = time()

        cprint(GREEN, f"\n Done running test {i+1}!")
        # TODO: Looks like runtime is inflated due to setup time? May need to go into param code
        # to put PCs in there. This will be called full_runtime?
        testcase_runtime = end - start
        print(f"Run time = {testcase_runtime}")
        # TODO: Verify that run succeeded
        write_runtime(output_log2, testcase_runtime)
        verify_testcase_status(output_log2)
        #write_test_log(output_log, runtime)
    test_suite_end = time()

    test_suite_runtime = test_suite_end - test_suite_start
    print(f"It took {test_suite_runtime} to run the entire test suite")
