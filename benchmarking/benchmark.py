import sys
from colors import *
from sort_prompts import *
from config_and_setup import *
from set_up_test import *
from post_processing import *

from time import time

import subprocess

# Save output logs
# TODO: Stop parsing in debug mode
# On error, don't parse
DEBUG = False
#RUN_SCRIPT = "spec_decoding_final_param.py"
# TODO: I need a better way of doing th is . . 
OUTPUT_DIR = "benchmarking/output_logs"
OUTPUT_DIR2 = OUTPUT_DIR.replace("benchmarking/", "")

CONFIG_FILE_LOC = "config.yaml"
if __name__ == "__main__":
    verify_venv()
    cprint(YELLOW, "\nGrabbing config file")
    config = parse_config_file(CONFIG_FILE_LOC)
    try:
        if config['demo_target']:
            run_script = "demo_real.py"
            run_prefix = "python3.9 -m torch.distributed.run"
        else:
            run_script = "spec_decoding_final_param.py"
            run_prefix = "deepspeed --num_gpus 1"
    except:
            run_script = "spec_decoding_final_param.py"
            run_prefix = "deepspeed --num_gpus 1"
    '''
    print(f"run_script = {run_script}")
    print(f"run_prefix = {run_prefix}")
    input()
    '''

    # TODO: Record time to run whole test suite
    # Construct input prompt list (get n input prompts of varying lengths) & point to new file in param code
    cprint(YELLOW, "\nConstructing inputs")
    filtered_input_prompts_file = create_input_prompts_from_config(config)
    update_input_file_in_param_code(filtered_input_prompts_file, run_script)
    # TODO: Update the number of speculative tokens generated by drafter
    # TODO: Print the above number for logging purposes

    cprint(YELLOW, "\nConstructing output results directory")
    OUTPUT_DIR2 = construct_output_directory(config, OUTPUT_DIR2)
    OUTPUT_DIR = os.path.join("benchmarking", OUTPUT_DIR2)

    # Construct test (or construct all tests at once and write into diff files?)
    # Run the test and collect its output; time runtime
    cprint(YELLOW, f"\nConstructing test suite")
    tests = create_test_suite(config)

    cprint(CYAN, "\nStarting tests")
    # TODO: Link test to test_name via class?
    test_suite_start = time()
    testcases_summary = []
    for i, testcase in enumerate(tests):
        testcase_num = i+1
        cprint(YELLOW, f"\nConstructing test {testcase_num} of {len(tests)}")
        test_info = construct_test(testcase, run_script)

        cprint(YELLOW, f"\nRunning test {testcase_num} of {len(tests)}")
        # TODO: Print test details
        #   - Drafter enabled? What are drafters, input prompt number, size of main model
        #test_name = "test2"
        test_name = config['test_name'] + str(testcase_num)
        output_log = os.path.join(OUTPUT_DIR, test_name) + ".log"
        output_error_log = os.path.join(OUTPUT_DIR, test_name + "_error") + ".log"
        output_log2 = os.path.join(OUTPUT_DIR2, test_name) + ".log"
        start = time()
        # TODO: Get ride of OUTPUT_DIR2 and just cd - to get back i nto directory
        cprint(MAGENTA, f"\tNum prompts: {config['num_input_prompts']}")
        if DEBUG:
            #subprocess.run(f"cd .. && deepspeed --num_gpus 1 {run_script} 2>{output_error_log}", shell=True)
            subprocess.run(f"cd .. && {run_prefix} {run_script} 2>{output_error_log}", shell=True)
            #subprocess.run(f"cd .. && deepspeed --num_gpus 1 {run_script}", shell=True)
        elif not DEBUG:
            subprocess.run(f"cd .. && {run_prefix} {run_script} >{output_log} 2>{output_error_log}", shell=True)
        # Figure out how to still print to terminal while piping
        #subprocess.run(f"cd .. && deepspeed --num_gpus 1 {run_script}", shell=True)
        #subprocess.run(f"cd .. && deepspeed --num_gpus 1 {run_script} >{output_log}", shell=True)
        #subprocess.run(f"cd .. && deepspeed --num_gpus 1 {run_script} > >(tee {output_log}) 2> >(tee {output_error_log} >&2)", shell=True)
        #subprocess.run(f"echo 'Return status: '$? >> {output_log2}", shell=True)
        end = time()

        cprint(GREEN, f"\n Done running test {testcase_num}!")



        # Log results and post processing
        # TODO: Looks like runtime is inflated due to setup time? May need to go into param code
        # to put PCs in there. This will be called full_runtime?
        testcase_runtime = end - start
        print(f"Run time = {testcase_runtime}")
        # TODO: Verify that run succeeded
        write_test_info(output_log2, test_info)
        if not DEBUG:
            write_runtime(output_log2, testcase_runtime)
            #write_test_log(output_log, runtime)
            testcases_summary = add_testcase_to_summary(testcase, output_log2, testcases_summary, config)
            if "demo_target" in config.keys():
                print("Checking demo_target")
                if config['demo_target'] == 1:
                    print("Confirmed demo_target = 1")

                    print("Entering run_average_script")
                    run_average_script("demo_real_output.txt", output_log.replace(".log", f"analyze_script_output{testcase_num}.log"))
                    print("Leaft run_average_script")
            # Update the summary csv after a testcase instead of waiting til all complete in case
            # we need to stop the program early
            summary_csv_loc = os.path.join(OUTPUT_DIR2, f"{config['test_name']}_summary.csv")
            testcases_summary_to_csv(testcases_summary, summary_csv_loc)
    test_suite_end = time()

    test_suite_runtime = test_suite_end - test_suite_start
    print(f"It took {test_suite_runtime} to run the entire test suite")
    runtime_file = os.path.join(OUTPUT_DIR2, config['test_name'])
    write_test_suite_runtime(test_suite_runtime, runtime_file)
