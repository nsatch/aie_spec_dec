#test_name: no_drafter_test_3
#test_name: no_draft_and_single_draft_test_8_topn_acceptance
#test_name: test_summary_write
#test_name: test_double_draft_large_small_swap
#test_name: test_analyze_output
test_name: real_sdraft_spec_token_sidebar

# Input prompts
# Don't point to an adjusted prompt file. We adjust inside the benchmark script based on size param below!
#full_prompt_input_file: "prompts/hellaswag.json"
#stride_enable: 1    # 0 for non-stride (grab top n prompts) 1 for stride (get distribution of diff lenghts)
#strided_prompt_output_file: "prompts/hellaswag_shortened.json"
#num_input_prompts: 8

#full_prompt_input_file: "prompts/hellaswag_sorted_acceptance.json"
full_prompt_input_file: "prompts/hellaswag.json"
stride_enable: 0    # 0 for non-stride (grab top n prompts) 1 for stride (get distribution of diff lenghts)
#strided_prompt_output_file: "prompts/hellaswag_sorted_acceptance.json"
strided_prompt_output_file: "prompts/hellaswag_shortened.json"
num_input_prompts: 10

# Type of experiment
# Left most are booleans indicating if that experiment is run
#   Next indent is the list of drafter models to use 
#   Pairs listed for double draft (larger draft model, smaller draft model)
baseline_no_draft: 
  active: 0
  # oracle_model: 'meta-llama/Llama-3.2-3B'
  #oracle_model: 'rasyosef/Llama-3.2-180M-Amharic'
  #oracle_model: 'meta-llama/Llama-2-13b'
  # oracle_model: 'meta-llama/Llama-2-70b-chat-hf'
  #oracle_model: 'minghaoyan/Wide-Sheared-LLaMA-290M'
  oracle_model: "meta-llama/Llama-2-70b-hf"

baseline_single_draft: 
  active: 1
  oracle_model: ['meta-llama/Llama-2-70b-hf']
  #draft1_model: ['minghaoyan/Wide-Sheared-LLaMA-796M', 'minghaoyan/Wide-Sheared-LLaMA-543M', 'minghaoyan/Wide-Sheared-LLaMA-290M']
  #draft1_model: ['minghaoyan/Wide-Sheared-LLaMA-1.3B']
  draft1_model: ['minghaoyan/Wide-Sheared-LLaMA-796M']
  #draft1_model: ['meta-llama/Llama-3.2-3B']

# TODO: Figure out how to group multiple tests together
double_draft:
  active: 0
  oracle_model: ['meta-llama/Llama-2-70b-hf']
  #oracle_model: ['meta-llama/Llama-2-13b']
  # The ith element of the draft1 and draft2 model are paired together for a single test. It is not a cross product 
  # of all elements in both lists!

  # Test to see if it makes more sense to use larger drafter model at the start or end of the prompt
  #draft1_model: ['minghaoyan/Wide-Sheared-LLaMA-796M', 'meta-llama/Llama-3.2-1.3B']
  draft1_model: ['minghaoyan/Wide-Sheared-LLaMA-796M']
  #draft1_model: ['meta-llama/Llama-3.2-1B']
  #draft2_model: ['meta-llama/Llama-3.2-1.3B', 'minghaoyan/Wide-Sheared-LLaMA-796M']
  #draft1_model: ['meta-llama/Llama-3.2-1B', minghaoyan/Wide-Sheared-LLaMA-290M', 'meta-llama/Llama-3.2-1B', 'minghaoyan/Wide-Sheared-LLaMA-290M']
  #draft2_model: ['minghaoyan/Wide-Sheared-LLaMA-290M', 'meta-llama/Llama-3.2-1B', 'meta-llama/Llama-3.2-1B',  'minghaoyan/Wide-Sheared-LLaMA-290M']
  drafter_switch_threshold: [25,50,75]

  # Test to see how sweeping the threshold affects the perf
  #draft1_model: ['minghaoyan/Wide-Sheared-LLaMA-796M']
  #draft2_model: ['minghaoyan/Wide-Sheared-LLaMA-290M']
  #drafter_switch_threshold: [50, 75]

#num_spec_tokens: [1, 5]
#num_spec_tokens: [3,7]
num_spec_tokens: [1,2,3]
#num_spec_tokens: [3, 7, 16, 32]
#num_spec_tokens: [1,2,3,4,5,7,15]

# Threhold is percent in the prompt

demo_target: 1
